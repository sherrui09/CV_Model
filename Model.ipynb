{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ecb22-45cc-4c6e-b433-f58aa6b12faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2wfvr8gi) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>binary_accuracy</td><td>▁▆▇██▇██▇████</td></tr><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_binary_accuracy</td><td>▁▆█▅▄▄▅▇▅▆▇▅▅</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▃▃▂▃▃▁▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>GFLOPs</td><td>195.74217</td></tr><tr><td>best_epoch</td><td>10</td></tr><tr><td>best_val_loss</td><td>5.2295</td></tr><tr><td>binary_accuracy</td><td>0.84817</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>loss</td><td>5.39502</td></tr><tr><td>val_binary_accuracy</td><td>0.84375</td></tr><tr><td>val_loss</td><td>5.48758</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">quiet-terrain-8</strong>: <a href=\"https://wandb.ai/doktoroso/myproject/runs/2wfvr8gi\" target=\"_blank\">https://wandb.ai/doktoroso/myproject/runs/2wfvr8gi</a><br/>Synced 5 W&B file(s), 1 media file(s), 16 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220908_155114-2wfvr8gi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2wfvr8gi). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tsteed/wandb/run-20220908_195523-38go7a8z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/doktoroso/myproject/runs/38go7a8z\" target=\"_blank\">good-butterfly-9</a></strong> to <a href=\"https://wandb.ai/doktoroso/myproject\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (2019, 9)\n",
      "train bbox shape: (7217, 6)\n",
      "test shape: (3, 3)\n",
      "ss shape: (3, 2)\n",
      "\n",
      "Number of samples in train and validation are 1817 and 201.\n",
      "Epoch 1/100\n",
      "151/151 [==============================] - ETA: 0s - loss: 10.0205 - binary_accuracy: 0.7973INFO:tensorflow:Assets written to: /home/tsteed/wandb/run-20220908_195523-38go7a8z/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Adding directory to artifact (/home/tsteed/wandb/run-20220908_195523-38go7a8z/files/model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 991s 7s/step - loss: 10.0205 - binary_accuracy: 0.7973 - val_loss: 6.0479 - val_binary_accuracy: 0.8301\n",
      "Epoch 2/100\n",
      "151/151 [==============================] - ETA: 0s - loss: 6.0205 - binary_accuracy: 0.8385INFO:tensorflow:Assets written to: /home/tsteed/wandb/run-20220908_195523-38go7a8z/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Adding directory to artifact (/home/tsteed/wandb/run-20220908_195523-38go7a8z/files/model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 984s 7s/step - loss: 6.0205 - binary_accuracy: 0.8385 - val_loss: 5.7312 - val_binary_accuracy: 0.8438\n",
      "Epoch 3/100\n",
      "151/151 [==============================] - ETA: 0s - loss: 5.6783 - binary_accuracy: 0.8458INFO:tensorflow:Assets written to: /home/tsteed/wandb/run-20220908_195523-38go7a8z/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Adding directory to artifact (/home/tsteed/wandb/run-20220908_195523-38go7a8z/files/model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 984s 7s/step - loss: 5.6783 - binary_accuracy: 0.8458 - val_loss: 5.5413 - val_binary_accuracy: 0.8451\n",
      "Epoch 4/100\n",
      "151/151 [==============================] - ETA: 0s - loss: 5.5960 - binary_accuracy: 0.8462INFO:tensorflow:Assets written to: /home/tsteed/wandb/run-20220908_195523-38go7a8z/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Adding directory to artifact (/home/tsteed/wandb/run-20220908_195523-38go7a8z/files/model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 980s 6s/step - loss: 5.5960 - binary_accuracy: 0.8462 - val_loss: 5.3079 - val_binary_accuracy: 0.8490\n",
      "Epoch 5/100\n",
      "151/151 [==============================] - 996s 7s/step - loss: 5.5663 - binary_accuracy: 0.8501 - val_loss: 5.3221 - val_binary_accuracy: 0.8548\n",
      "Epoch 6/100\n",
      "151/151 [==============================] - 991s 7s/step - loss: 5.4639 - binary_accuracy: 0.8466 - val_loss: 5.5041 - val_binary_accuracy: 0.8405\n",
      "Epoch 7/100\n",
      "151/151 [==============================] - 987s 7s/step - loss: 5.4489 - binary_accuracy: 0.8496 - val_loss: 5.3081 - val_binary_accuracy: 0.8470\n",
      "Epoch 8/100\n",
      "151/151 [==============================] - ETA: 0s - loss: 5.4185 - binary_accuracy: 0.8500INFO:tensorflow:Assets written to: /home/tsteed/wandb/run-20220908_195523-38go7a8z/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Adding directory to artifact (/home/tsteed/wandb/run-20220908_195523-38go7a8z/files/model-best)... Done. 0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 1001s 7s/step - loss: 5.4185 - binary_accuracy: 0.8500 - val_loss: 5.3069 - val_binary_accuracy: 0.8451\n",
      "Epoch 9/100\n",
      "151/151 [==============================] - 991s 7s/step - loss: 5.4138 - binary_accuracy: 0.8508 - val_loss: 5.3909 - val_binary_accuracy: 0.8490\n",
      "Epoch 10/100\n",
      "151/151 [==============================] - 993s 7s/step - loss: 5.4080 - binary_accuracy: 0.8480 - val_loss: 5.4897 - val_binary_accuracy: 0.8431\n",
      "Epoch 11/100\n",
      "151/151 [==============================] - 1000s 7s/step - loss: 5.3639 - binary_accuracy: 0.8460 - val_loss: 5.6197 - val_binary_accuracy: 0.8392\n",
      "Epoch 12/100\n",
      "151/151 [==============================] - 974s 6s/step - loss: 5.3020 - binary_accuracy: 0.8477 - val_loss: 5.6492 - val_binary_accuracy: 0.8470\n",
      "Epoch 13/100\n",
      "151/151 [==============================] - 985s 7s/step - loss: 5.3480 - binary_accuracy: 0.8496 - val_loss: 5.6335 - val_binary_accuracy: 0.8451\n",
      "Epoch 14/100\n",
      "151/151 [==============================] - 990s 7s/step - loss: 5.3776 - binary_accuracy: 0.8458 - val_loss: 5.7991 - val_binary_accuracy: 0.8438\n",
      "Epoch 15/100\n",
      "151/151 [==============================] - 994s 7s/step - loss: 5.3521 - binary_accuracy: 0.8478 - val_loss: 5.6520 - val_binary_accuracy: 0.8431\n",
      "Epoch 16/100\n",
      "151/151 [==============================] - 995s 7s/step - loss: 5.3240 - binary_accuracy: 0.8462 - val_loss: 5.8165 - val_binary_accuracy: 0.8451\n",
      "Epoch 17/100\n",
      "151/151 [==============================] - 1005s 7s/step - loss: 5.3241 - binary_accuracy: 0.8497 - val_loss: 5.6215 - val_binary_accuracy: 0.8496\n",
      "Epoch 18/100\n",
      "151/151 [==============================] - 990s 7s/step - loss: 5.3267 - binary_accuracy: 0.8513 - val_loss: 5.6737 - val_binary_accuracy: 0.8444\n",
      "Epoch 19/100\n",
      "151/151 [==============================] - 991s 7s/step - loss: 5.3026 - binary_accuracy: 0.8501 - val_loss: 5.3531 - val_binary_accuracy: 0.8490\n",
      "Epoch 20/100\n",
      "151/151 [==============================] - 985s 7s/step - loss: 5.3257 - binary_accuracy: 0.8506 - val_loss: 5.7735 - val_binary_accuracy: 0.8424\n",
      "Epoch 21/100\n",
      "151/151 [==============================] - 984s 7s/step - loss: 5.2713 - binary_accuracy: 0.8516 - val_loss: 5.3501 - val_binary_accuracy: 0.8464\n",
      "Epoch 22/100\n",
      "151/151 [==============================] - 989s 7s/step - loss: 5.2849 - binary_accuracy: 0.8471 - val_loss: 5.5223 - val_binary_accuracy: 0.8581\n",
      "Epoch 23/100\n",
      "151/151 [==============================] - 982s 6s/step - loss: 5.3536 - binary_accuracy: 0.8504 - val_loss: 6.0198 - val_binary_accuracy: 0.8477\n",
      "Epoch 24/100\n",
      "151/151 [==============================] - 990s 7s/step - loss: 5.2824 - binary_accuracy: 0.8491 - val_loss: 5.5952 - val_binary_accuracy: 0.8418\n",
      "Epoch 25/100\n",
      "151/151 [==============================] - 982s 6s/step - loss: 5.3162 - binary_accuracy: 0.8492 - val_loss: 5.8374 - val_binary_accuracy: 0.8418\n",
      "Epoch 26/100\n",
      "151/151 [==============================] - 993s 7s/step - loss: 5.3051 - binary_accuracy: 0.8475 - val_loss: 5.7484 - val_binary_accuracy: 0.8509\n",
      "Epoch 27/100\n",
      "151/151 [==============================] - 983s 7s/step - loss: 5.2872 - binary_accuracy: 0.8524 - val_loss: 5.3516 - val_binary_accuracy: 0.8444\n",
      "Epoch 28/100\n",
      "151/151 [==============================] - 974s 6s/step - loss: 5.2720 - binary_accuracy: 0.8488 - val_loss: 5.8110 - val_binary_accuracy: 0.8411\n",
      "Epoch 29/100\n",
      "151/151 [==============================] - 976s 6s/step - loss: 5.2651 - binary_accuracy: 0.8491 - val_loss: 5.5477 - val_binary_accuracy: 0.8451\n",
      "Epoch 30/100\n",
      "151/151 [==============================] - 1001s 7s/step - loss: 5.2764 - binary_accuracy: 0.8500 - val_loss: 5.6632 - val_binary_accuracy: 0.8496\n",
      "Epoch 31/100\n",
      "151/151 [==============================] - 1037s 7s/step - loss: 5.2649 - binary_accuracy: 0.8484 - val_loss: 5.7380 - val_binary_accuracy: 0.8372\n",
      "Epoch 32/100\n",
      "151/151 [==============================] - 1000s 7s/step - loss: 5.2822 - binary_accuracy: 0.8493 - val_loss: 5.7488 - val_binary_accuracy: 0.8503\n",
      "Epoch 33/100\n",
      "151/151 [==============================] - 1023s 7s/step - loss: 5.2724 - binary_accuracy: 0.8496 - val_loss: 5.5708 - val_binary_accuracy: 0.8424\n",
      "Epoch 34/100\n",
      "151/151 [==============================] - 1083s 7s/step - loss: 5.2663 - binary_accuracy: 0.8522 - val_loss: 5.4396 - val_binary_accuracy: 0.8346\n",
      "Epoch 35/100\n",
      "151/151 [==============================] - 1081s 7s/step - loss: 5.2954 - binary_accuracy: 0.8480 - val_loss: 5.6804 - val_binary_accuracy: 0.8496\n",
      "Epoch 36/100\n",
      "151/151 [==============================] - 1054s 7s/step - loss: 5.3049 - binary_accuracy: 0.8482 - val_loss: 5.5476 - val_binary_accuracy: 0.8490\n",
      "Epoch 37/100\n",
      "151/151 [==============================] - 1064s 7s/step - loss: 5.3319 - binary_accuracy: 0.8509 - val_loss: 5.7751 - val_binary_accuracy: 0.8490\n",
      "Epoch 38/100\n",
      "151/151 [==============================] - 1076s 7s/step - loss: 5.2470 - binary_accuracy: 0.8486 - val_loss: 5.5379 - val_binary_accuracy: 0.8464\n",
      "Epoch 39/100\n",
      " 12/151 [=>............................] - ETA: 14:34 - loss: 5.0974 - binary_accuracy: 0.8524"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import torchio as tio\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.init(project=\"myproject\")\n",
    "\n",
    "try:\n",
    "\n",
    "    with tf.device('/device:GPU:5'):\n",
    "        \n",
    "# Runs the op.\n",
    "\n",
    "            \n",
    "        demo = False\n",
    "        process = False\n",
    "        #UR DATA DIR\n",
    "        data_dir   = '/opt/localdata/CervicalSpineKaggle/'\n",
    "        #DESIRED OUTPUT DIR\n",
    "        out_dir    = '/opt/localdata/CervicalSpineKaggle/Process/'\n",
    "\n",
    "        if process:\n",
    "\n",
    "            for dataset in ['train_images']:\n",
    "                dataset_dir = f'{data_dir}{dataset}'\n",
    "                patients = os.listdir(dataset_dir)\n",
    "                if demo:\n",
    "                    ##FIRST 5 SUBS ONLY\n",
    "                    patients = patients[:2]\n",
    "                \n",
    "                print(f'Total patients in {dataset} dataset: {len(patients)}')\n",
    "\n",
    "                count = 0\n",
    "                for patient in patients:\n",
    "                    count = count + 1\n",
    "                    print(f'{dataset}: {count}/{len(patients)}')\n",
    "                    scan_src  = f'{dataset_dir}/{patient}/'\n",
    "                    scan_dest = f'{out_dir}/{dataset}/{patient}'\n",
    "                    #Path(scan_dest).mkdir(parents=True, exist_ok=True)\n",
    "                    image = tio.ScalarImage(scan_src)\n",
    "                    HOUNSFIELD_AIR, HOUNSFIELD_BONE = -1000, 1000\n",
    "                    clamp=tio.Clamp(out_min=HOUNSFIELD_AIR, out_max=HOUNSFIELD_BONE)\n",
    "                    image=clamp(image)\n",
    "                    transforms = [\n",
    "                        ##Standardize Orientation\n",
    "                        tio.ToCanonical(),\n",
    "                        ##Resample to 1mm isotropic voxels bspline\n",
    "                        tio.Resample(1),           \n",
    "                        ## Crop or pad to 140/140/220\n",
    "                        tio.CropOrPad((160,160,224)),\n",
    "                        ##Rescale from -1 to 1 at -1000 to 1000 HU\n",
    "                        tio.RescaleIntensity(out_min_max = [0,1], in_min_max = [0, 800])\n",
    "                    ]\n",
    "                    transform = tio.Compose(transforms)\n",
    "                    preprocessed = transform(image)\n",
    "                    preprocessed.save(f'{scan_dest}.nii.gz')\n",
    "                    \n",
    "        import random\n",
    "\n",
    "        def tio_augment(vol_base):\n",
    "            \n",
    "            transforms_dict = {\n",
    "            tio.RandomAffine(scales=(0.90, 1.1),degrees=(-25,25)): 0.33,\n",
    "            tio.RandomElasticDeformation(num_control_points=(8, 8, 8),locked_borders=2): 0.33,\n",
    "            tio.RandomFlip(axes=('LR',)): 0.33,\n",
    "            } \n",
    "            transform = tio.OneOf(transforms_dict)\n",
    "            transformed_b = transform(vol_base)\n",
    "            transformed_b = transformed_b.data\n",
    "        \n",
    "            return transformed_b\n",
    "\n",
    "        ##This is the augmentation function workhorse\n",
    "        def tio_augment_wlabel(subj):\n",
    "            transforms_dict = {\n",
    "            tio.RandomAffine(scales=(0.90, 1.1),degrees=(-45,45)): 0.5,\n",
    "            tio.RandomElasticDeformation(num_control_points=(8, 8, 8),locked_borders=2): 0.5,\n",
    "            } \n",
    "            transform = tio.OneOf(transforms_dict)\n",
    "            subj_aug = transform(subj)\n",
    "            transforms_dictflip = {\n",
    "            tio.RandomFlip(axes=('LR',)): 0.25,\n",
    "            tio.RandomFlip(axes=('IS',)): 0.5,\n",
    "            tio.RandomFlip(axes=('AP',)): 0.25,\n",
    "        } \n",
    "            transform = tio.OneOf(transforms_dictflip)\n",
    "            subj_aug = transform(subj_aug)\n",
    "            transform = tio.Clamp(out_min=0, out_max=1)\n",
    "            subj_aug = transform(subj_aug)\n",
    "            return subj_aug\n",
    "\n",
    "        def read_nifti_file(filepath):\n",
    "            scan = tio.ScalarImage(filepath)\n",
    "            return scan\n",
    "\n",
    "        def train_augmentation_wlabel(path_base, path_label):\n",
    "            subj = tio.Subject(\n",
    "            base=tio.ScalarImage(path_base),\n",
    "            label=tio.LabelMap(path_label)\n",
    "            )\n",
    "            subj_aug= tio_augment_wlabel(subj)\n",
    "            aug_vol_b=subj_aug.base\n",
    "            aug_vol_l=subj_aug.label\n",
    "            aug_vol_b=np.squeeze(aug_vol_b.data)\n",
    "            aug_vol_l=np.squeeze(aug_vol_l.data)\n",
    "            return aug_vol_b, aug_vol_l\n",
    "\n",
    "        def validation_wlabel(path_base, path_label):\n",
    "            subj_aug = tio.Subject(\n",
    "            base=tio.ScalarImage(path_base),\n",
    "            label=tio.LabelMap(path_label)\n",
    "            )\n",
    "            aug_vol_b=subj_aug.base\n",
    "            aug_vol_l=subj_aug.label\n",
    "            aug_vol_b=np.squeeze(aug_vol_b.data)\n",
    "            aug_vol_l=np.squeeze(aug_vol_l.data)\n",
    "            return aug_vol_b, aug_vol_l\n",
    "\n",
    "        ##LOSS FUNCTIONS\n",
    "        def dice_coe(y_true,y_pred, loss_type='jaccard', smooth=1.):\n",
    "            y_true_f = tf.reshape(y_true,[-1])\n",
    "            y_pred_f = tf.reshape(y_pred,[-1])\n",
    "            intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "            if loss_type == 'jaccard':\n",
    "                union = tf.reduce_sum(tf.square(y_pred_f)) + tf.reduce_sum(tf.square(y_true_f))\n",
    "            elif loss_type == 'sorensen':\n",
    "                union = tf.reduce_sum(y_pred_f) + tf.reduce_sum(y_true_f)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown `loss_type`: %s\" % loss_type)\n",
    "            return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "        def dice_loss(y_true,y_pred, loss_type='jaccard', smooth=1.):\n",
    "            y_true_f = tf.cast(tf.reshape(y_true,[-1]),tf.float32)\n",
    "            y_pred_f =tf.cast(tf.reshape(y_pred,[-1]),tf.float32)\n",
    "            intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "            if loss_type == 'jaccard':\n",
    "                union = tf.reduce_sum(tf.square(y_pred_f)) + tf.reduce_sum(tf.square(y_true_f))\n",
    "            elif loss_type == 'sorensen':\n",
    "                union = tf.reduce_sum(y_pred_f) + tf.reduce_sum(y_true_f)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown `loss_type`: %s\" % loss_type)\n",
    "            return (1-(2. * intersection + smooth) / (union + smooth))\n",
    "\n",
    "        def read_nifti_file(filepath):\n",
    "            scan = tio.ScalarImage(filepath)\n",
    "            return scan\n",
    "\n",
    "        class DataGenerator(keras.utils.Sequence):\n",
    "            'Generates data for Keras'\n",
    "            def __init__(self, list_scans, batch_size, dim,\n",
    "                        n_classes, shuffle):\n",
    "                'Initialization'\n",
    "                self.dim = dim\n",
    "                self.batch_size = batch_size\n",
    "                self.list_scans = list_scans\n",
    "                self.n_classes = n_classes\n",
    "                self.shuffle = shuffle\n",
    "                self.on_epoch_end()\n",
    "\n",
    "            def __len__(self):\n",
    "                'Denotes the number of batches per epoch'\n",
    "                return int(np.floor(len(self.list_scans) / self.batch_size))\n",
    "\n",
    "            def __getitem__(self, index):\n",
    "                'Generate one batch of data'\n",
    "                # Generate indexes of the batch\n",
    "                indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "                # Find list of IDs\n",
    "                list_scans_temp = [self.list_scans[k] for k in indexes]\n",
    "                #labels_temp=[self.labels[k] for k in indexes]\n",
    "                \n",
    "                # Generate data\n",
    "                #X, y = self.__data_generation(list_scans_temp,labels_temp)\n",
    "                X= self.__data_generation(list_scans_temp)\n",
    "                return X\n",
    "                #return X,y\n",
    "\n",
    "            def on_epoch_end(self):\n",
    "                'Updates indexes after each epoch'\n",
    "                self.indexes = np.arange(len(self.list_scans))\n",
    "                if self.shuffle == True:\n",
    "                    np.random.shuffle(self.indexes)\n",
    "\n",
    "            #def __data_generation(self, list_scans_temp,labels_temp):\n",
    "            def __data_generation(self, list_scans_temp):\n",
    "                'Generates data containing batch_size samples' # X : (n_samples, *dim)\n",
    "                # Initialization\n",
    "                X = np.empty((self.batch_size, *self.dim))\n",
    "                #y = np.empty((self.batch_size, *self.dim))\n",
    "                # Generate data\n",
    "                for i, ID in enumerate(list_scans_temp):\n",
    "                    # Store Image\n",
    "                # Xtmp, ytmp= read_nifti_file(ID,labels_temp[i])\n",
    "                    Xtmp= read_nifti_file(ID)\n",
    "                    #Xtmp=tio.ToCanonical()(Xtmp)\n",
    "                    #transform = tio.Compose(transforms)\n",
    "                    #Xtmp = transform(Xtmp)\n",
    "                    Xtmp=np.squeeze(Xtmp.data)\n",
    "                    X[i]=Xtmp\n",
    "                    #y[i]=ytmp\n",
    "\n",
    "                return X\n",
    "            \n",
    "        seg_convert=False\n",
    "        patients =glob(f\"{out_dir}train_images/*.nii.gz\")\n",
    "        count=0\n",
    "\n",
    "        d=[]\n",
    "        if seg_convert:\n",
    "            try:\n",
    "                with tf.device('/device:GPU:2'):\n",
    "        #params = {'batch_size': 1,\n",
    "        #        'dim': (160,160,224),\n",
    "        #        'n_classes': 9,\n",
    "        #        'shuffle': False}\n",
    "                \n",
    "        #test_generator= DataGenerator(patients, **params)\n",
    "                    model_3d_unet=keras.models.load_model('/home/tsteed/RSNA_CHALLENGE/3dUnet_save.h5', custom_objects={'dice_loss': dice_loss, 'dice_coe': dice_coe })             \n",
    "                    for patient in patients:\n",
    "                        count = count + 1\n",
    "                        print(f'Segmenting   {patient}...: {count}/{len(patients)}')\n",
    "                \n",
    "                        #model_3d_unet=keras.models.load_model('/home/tsteed/RSNA_CHALLENGE/3dUnet_save.h5', custom_objects={'dice_loss': dice_loss, 'dice_coe': dice_coe })  \n",
    "                        #patients = patients[:10]\n",
    "                        x=read_nifti_file(patient)\n",
    "                        y=x.data\n",
    "                        y=np.array(y)\n",
    "                \n",
    "        \n",
    "                        out2=model_3d_unet.predict(y)\n",
    "                        y=x\n",
    "                        y.data=out2[:,:,:,:,0]\n",
    "                        shortpath=patient.split('/')[-1]\n",
    "                        suid=(shortpath[:len(shortpath)-7])\n",
    "                        save_location=f\"/opt/localdata/CervicalSpineKaggle/pred_segs/{suid}.nii.gz\"\n",
    "                        d.append(save_location)\n",
    "                        y.save(save_location)\n",
    "                    \n",
    "            except RuntimeError as e:\n",
    "                    print(e)\n",
    "\n",
    "        segs_patients =glob(f\"{data_dir}pred_segs/*.nii.gz\")\n",
    "\n",
    "\n",
    "        class Channel_3D_DataGenerator(keras.utils.Sequence):\n",
    "            'Generates data for Keras'\n",
    "            def __init__(self, list_scans, list_scans2, labels, batch_size, dim,\n",
    "                        n_classes, shuffle):\n",
    "                'Initialization'\n",
    "                self.dim = dim\n",
    "                self.batch_size = batch_size\n",
    "                self.labels = labels\n",
    "                self.list_scans = list_scans\n",
    "                self.list_scans2 = list_scans2\n",
    "                self.n_classes = n_classes\n",
    "                self.shuffle = shuffle\n",
    "                self.on_epoch_end()\n",
    "\n",
    "            def __len__(self):\n",
    "                'Denotes the number of batches per epoch'\n",
    "                return int(np.floor(len(self.list_scans) / self.batch_size))\n",
    "\n",
    "            def __getitem__(self, index):\n",
    "                'Generate one batch of data'\n",
    "                # Generate indexes of the batch\n",
    "                indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "                #indexes = self.indexes[index:(index+(1*self.batch_size))]\n",
    "                # Find list of IDs\n",
    "                list_scans_temp = [self.list_scans[k] for k in indexes]\n",
    "                list_scans2_temp=[self.list_scans2[k] for k in indexes]\n",
    "                labels_temp=[self.labels[k] for k in indexes]\n",
    "                \n",
    "\n",
    "                # Generate data\n",
    "                [X, z], y = self.__data_generation(list_scans_temp, list_scans2_temp,labels_temp)\n",
    "\n",
    "                return [X,z],y\n",
    "\n",
    "            def on_epoch_end(self):\n",
    "                'Updates indexes after each epoch'\n",
    "                self.indexes = np.arange(len(self.list_scans))\n",
    "                if self.shuffle == True:\n",
    "                    np.random.shuffle(self.indexes)\n",
    "\n",
    "            def __data_generation(self, list_scans_temp, list_scans2_temp,labels_temp):\n",
    "                'Generates data containing batch_size samples' # X : (n_samples, *dim, )\n",
    "                # Initialization\n",
    "                X = np.empty((self.batch_size, *self.dim,1))\n",
    "                y = np.empty((self.batch_size,self.n_classes), dtype=int)\n",
    "                z = np.empty((self.batch_size, *self.dim,1))\n",
    "                # Generate data\n",
    "                for i, ID in enumerate(list_scans_temp):\n",
    "                    # Store Image\n",
    "                    #X[i,], z[i,] = train_augmentation_wlabel(ID, list_scans2_temp[i])\n",
    "                    babx, babz = train_augmentation_wlabel(ID, list_scans2_temp[i])\n",
    "                    \n",
    "                    X[i,]=np.expand_dims(babx, axis=3)\n",
    "                    z[i,]=np.expand_dims(babz, axis=3)\n",
    "                    # Store class\n",
    "                    y[i] = labels_temp[i]\n",
    "\n",
    "                return [X,z],y\n",
    "            \n",
    "        # Load metadata\n",
    "\n",
    "\n",
    "        train_df = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n",
    "        train_bbox = pd.read_csv(os.path.join(data_dir,\"train_bounding_boxes.csv\"))\n",
    "        test_df = pd.read_csv(os.path.join(data_dir,\"test.csv\"))\n",
    "        ss = pd.read_csv(os.path.join(data_dir,\"sample_submission.csv\"))\n",
    "\n",
    "        # Print dataframe shapes\n",
    "        print('train shape:', train_df.shape)\n",
    "        print('train bbox shape:', train_bbox.shape)\n",
    "        print('test shape:', test_df.shape)\n",
    "        print('ss shape:', ss.shape)\n",
    "        print('')\n",
    "        \n",
    "\n",
    "        #out_dir = \"/media/doktoroso/Exos18/gyrixis/projects/RSNA_Spine_processed\"\n",
    "        x = np.array(patients)\n",
    "        z = np.array(segs_patients)\n",
    "\n",
    "        unx,unidx=np.unique(x, return_index=True)\n",
    "        indices = np.random.permutation(len(unx))\n",
    "        training_idx, test_idx = indices[:round(0.9*len(indices))], indices[round(0.9*len(indices))+1:]\n",
    "        training_idx = np.in1d(x, unx[training_idx])\n",
    "        test_idx=np.in1d(x, unx[test_idx])\n",
    "        training_idx.astype(int)\n",
    "        x_train, x_val = x[training_idx], x[test_idx]\n",
    "        z_train, z_val = z[training_idx], z[test_idx]\n",
    "\n",
    "        res=[]\n",
    "        for study in train_df['StudyInstanceUID']:\n",
    "            res.append(os.path.join(out_dir, \"train_images\", study, \".nii.gz\"))\n",
    "\n",
    "\n",
    "        train_df['image_path']=res\n",
    "        ## Index train_DF for overal fracture ==> C7 binary to create labels\n",
    "        y=np.array(train_df.iloc[:,[1,2,3,4,5,6,7,8]])\n",
    "        #y=np.array(train_df.iloc[:,[1]])\n",
    "\n",
    "\n",
    "        y_train, y_val = y[training_idx], y[test_idx]\n",
    "        print(\n",
    "            \"Number of samples in train and validation are %d and %d.\"\n",
    "            % (y_train.shape[0], y_val.shape[0])\n",
    "        )\n",
    "\n",
    "\n",
    "        #setnumberofprediction labels\n",
    "        nclasses=y_train.shape[1]\n",
    "\n",
    "        # Generators\n",
    "        ## Parameters\n",
    "        params = {'batch_size': 12,\n",
    "                'dim': (160,160,224),\n",
    "                'n_classes': nclasses,\n",
    "                'shuffle': True}\n",
    "\n",
    "        training_generator = Channel_3D_DataGenerator(x_train, z_train, y_train, **params)\n",
    "        validation_generator= Channel_3D_DataGenerator(x_val, z_val, y_val, **params)\n",
    "\n",
    "\n",
    "\n",
    "    #kernel_initializer='he_normal'\n",
    "    #from keras.regularizers import l2\n",
    "    #kernel_regularizer=l2(1e-4)\n",
    "    #CHANNEL_AXIS = 3\n",
    "        def resnet_3d():\n",
    "            #kernel_initializer='he_normal'\n",
    "            from keras.regularizers import l2\n",
    "            kernel_regularizer=l2(1e-4)\n",
    "            width=160\n",
    "            height=160\n",
    "            depth=224\n",
    "        #length=z_train.shape[1]\n",
    "            kernel_initializer='he_normal'\n",
    "\n",
    "            inputimg = tf.keras.Input((width, height, depth, 1), name=\"inputimg\")\n",
    "            inputseg = tf.keras.Input((width, height, depth, 1), name=\"inputseg\")\n",
    "        \n",
    "            xseg = layers.Lambda(lambda x: x * 0.1)(inputseg)\n",
    "            #masterin=layers.Concatenate()([inputimg, inputseg])\n",
    "            masterin=layers.Multiply()([inputimg,xseg])\n",
    "            ## First layer\n",
    "            conv1 = layers.Conv3D(filters=32, kernel_size=(5, 5, 5),\n",
    "                            strides=(2,2,2), padding=\"same\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=kernel_regularizer\n",
    "                            )(inputimg)\n",
    "            #print(conv1.shape)\n",
    "            conv11 = layers.Conv3D(filters=32, kernel_size=(5, 5, 5),\n",
    "                            strides=(1,1,1), padding=\"same\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=kernel_regularizer\n",
    "                            )(conv1)\n",
    "        \n",
    "            norm1 = layers.BatchNormalization(axis=-1)(conv11)\n",
    "            relu1 = layers.Activation(\"relu\")(norm1)\n",
    "            #print(relu1.shape)\n",
    "            residual1 = layers.Conv3D(filters=32, kernel_size=(3, 3, 3),\n",
    "                            strides=(1,1,1), padding=\"same\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=kernel_regularizer\n",
    "                            )(relu1)\n",
    "            #print(residual1.shape)\n",
    "            resblock1 = layers.add([conv1, residual1])\n",
    "        \n",
    "            conv2 = layers.Conv3D(filters=64, kernel_size=(5, 5, 5),\n",
    "                            strides=(2,2,2), padding=\"same\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=kernel_regularizer\n",
    "                            )(resblock1)\n",
    "        \n",
    "            conv22 = layers.Conv3D(filters=64, kernel_size=(5, 5, 5),\n",
    "                            strides=(1,1,1), padding=\"same\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=kernel_regularizer\n",
    "                            )(conv2)\n",
    "\n",
    "        \n",
    "            norm2 = layers.BatchNormalization(axis=-1)(conv22)\n",
    "            relu2 = layers.Activation(\"relu\")(norm2)\n",
    "            #print(relu1.shape)\n",
    "            residual2 = layers.Conv3D(filters=64, kernel_size=(3, 3, 3),\n",
    "                            strides=(1,1,1), padding=\"same\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=kernel_regularizer\n",
    "                            )(relu2)\n",
    "            #print(residual1.shape)\n",
    "            resblock2 = layers.add([conv2, residual2])\n",
    "        \n",
    "        \n",
    "            conv3 = layers.Conv3D(filters=64, kernel_size=(3, 3, 3),\n",
    "                            strides=(2,2,2), padding=\"same\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=kernel_regularizer\n",
    "                            )(resblock2)\n",
    "        \n",
    "            conv33 = layers.Conv3D(filters=128, kernel_size=(3, 3, 3),\n",
    "                            strides=(1,1,1), padding=\"same\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=kernel_regularizer\n",
    "                            )(conv3)\n",
    "\n",
    "        \n",
    "            norm3 = layers.BatchNormalization(axis=-1)(conv3)\n",
    "            relu3 = layers.Activation(\"relu\")(norm3)\n",
    "            #print(relu1.shape)\n",
    "            residual3 = layers.Conv3D(filters=64, kernel_size=(3, 3, 3),\n",
    "                            strides=(1,1,1), padding=\"same\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=kernel_regularizer\n",
    "                            )(relu3)\n",
    "        #print(residual1.shape)\n",
    "            resblock3 = layers.add([conv3, residual3])\n",
    "        \n",
    "            conv4 = layers.Conv3D(filters=16, kernel_size=(3, 3, 3),\n",
    "                            strides=(2,2,2), padding=\"same\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=kernel_regularizer\n",
    "                            )(resblock3)\n",
    "        \n",
    "            conv44 = layers.Conv3D(filters=16, kernel_size=(3, 3, 3),\n",
    "                            strides=(1,1,1), padding=\"same\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=kernel_regularizer\n",
    "                            )(conv4)\n",
    "\n",
    "        \n",
    "            norm4 = layers.BatchNormalization(axis=-1)(conv44)\n",
    "            relu4 = layers.Activation(\"relu\")(norm4)\n",
    "            #print(relu1.shape)\n",
    "            residual4 = layers.Conv3D(filters=16, kernel_size=(3, 3, 3),\n",
    "                            strides=(1,1,1), padding=\"same\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=kernel_regularizer\n",
    "                            )(relu4)\n",
    "            #print(residual1.shape)\n",
    "            resblock4 = layers.add([conv4, residual4])\n",
    "        \n",
    "            conv5 = layers.Conv3D(filters=16, kernel_size=(3, 3, 3),\n",
    "                            strides=(2,2,1), padding=\"same\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                            kernel_regularizer=kernel_regularizer\n",
    "                            )(resblock4)\n",
    "        \n",
    "        \n",
    "    #     #cnn = Conv3D(32, kernel_size=(5), strides=(2), padding='same', activation='relu', kernel_initializer='he_uniform', name=\"conv_3d_1\")(img_input)\n",
    "    #     cnn = MaxPooling3D(pool_size=(2,2,2), strides=(2))(cnn)\n",
    "    #     cnn = Conv3D(64, kernel_size=(3), strides=(2), padding='same', activation='relu', kernel_initializer='he_uniform', name=\"conv_3d_2\")(img_input)\n",
    "    #     cnn = MaxPooling3D(pool_size=(2,2,2), strides=(2, 2, 2))(cnn)\n",
    "            flatten = layers.Flatten()(conv5)\n",
    "            x = layers.Dense(512, activation='relu')(flatten)\n",
    "            x = layers.Dropout(0.4)(x)\n",
    "            x = layers.Dense(512, activation='relu')(x)\n",
    "            outputs = layers.Dense(units=nclasses,activation=\"sigmoid\")(x)\n",
    "            model = keras.Model(\n",
    "            inputs=[inputimg, inputseg],\n",
    "            outputs=[outputs],\n",
    "            )\n",
    "            return model\n",
    "\n",
    "\n",
    "        model = resnet_3d()\n",
    "\n",
    "    ##NIFTY WAY TO CALCULATE CLASS WEIGHTS BY PROBABILITY\n",
    "    #from sklearn.utils.class_weight import compute_class_weight\n",
    "    #y_integers = np.argmax(y_train, axis=1)\n",
    "    #class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "    #d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    ## UNCOMMENT FOR FUN BINARY METRICS\n",
    "    #metrics = [\n",
    "    #      keras.metrics.TruePositives(name='tp'),\n",
    "    #      keras.metrics.FalsePositives(name='fp'),\n",
    "    #      keras.metrics.TrueNegatives(name='tn'),\n",
    "    #      keras.metrics.FalseNegatives(name='fn'), \n",
    "    #      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    #     keras.metrics.Precision(name='precision'),\n",
    "    #      keras.metrics.Recall(name='recall'),\n",
    "    #      keras.metrics.AUC(name='auc'),\n",
    "    #      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    #]\n",
    "        \n",
    "        \n",
    "        #def comp_loss(y_true, y_pred):\n",
    "            \n",
    "        #    y_true=tf.squeeze(y_true)\n",
    "        #    y_pred=tf.squeeze(y_pred)\n",
    "        #    temp_true=tf.cast(y_true, tf.float32)\n",
    "        #    added=tf.add(temp_true,y_pred)-1\n",
    "        #    addedp = tf.cast(added > 0, added.dtype) *2 \n",
    "        #    addedn = tf.cast(added < 0, added.dtype) *1 \n",
    "        #    added=tf.add(addedp,addedn)\n",
    "        #    value = float(7*tf.gather(added, 0))\n",
    "        #    added=tf.tensor_scatter_nd_update(added, [0], [value])\n",
    "        #    return float(1. / tf.math.reduce_sum(added, axis=None, keepdims=False, name=None))\n",
    "    \n",
    "        weights = [14, 2, 2, 2, 2, 2, 2, 2]\n",
    "\n",
    "        initial_learning_rate = 0.0001\n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "            initial_learning_rate, decay_steps=100000, decay_rate=0.99, staircase=True\n",
    "            )\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.BinaryCrossentropy(\n",
    "            from_logits=False),\n",
    "            loss_weights=weights,\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "            metrics=[\"binary_accuracy\"],\n",
    "    # metrics=[\"binary_accuracy\"],\n",
    "        #metrics=metrics,\n",
    "        )\n",
    "       # class_weights = {0: 0.1, 1: 2}\n",
    "        #Define callbacks.\n",
    "        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "            \"3d_rsna_resnet.h5\", save_best_only=True\n",
    "            )\n",
    "        early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_binary_accuracy\", patience=50)\n",
    "    # Train the model, doing validation at the end of each epoch\n",
    "\n",
    "        model.fit(\n",
    "            training_generator,\n",
    "            validation_data=validation_generator,\n",
    "            callbacks=[checkpoint_cb, early_stopping_cb, WandbCallback()],\n",
    "            #class_weight=class_weights,\n",
    "            epochs=100\n",
    "        )\n",
    "\n",
    "        model.save(\"3d_rsna_resnet.h5\")\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297924ce-fce2-4955-9f85-30872acceeae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21edb52b-53ed-4f2a-ba3d-6d1812ff05dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet_3d()\n",
    "\n",
    "model.compile(\n",
    "loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "     metrics=[\"binary_accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0698a8c-1175-4d69-b97b-e32dafbfee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:6'):\n",
    "    model = resnet_3d()\n",
    "\n",
    "    model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=[\"binary_accuracy\"]\n",
    "    )\n",
    "    \n",
    "    model.load_weights('/home/tsteed/wandb/latest-run/files/model-best.h5')\n",
    "    a=training_generator.__getitem__(10)\n",
    "    b=a[0]\n",
    "    prediction = model.predict(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f11f2-56ad-40ea-ac18-ef6712c78595",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=training_generator.__getitem__(10)\n",
    "b=a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf622d5-2f28-4f01-85cd-fbbd0a3cb5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prediction = model.predict(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e888a2-cdde-4547-be95-18f4326f9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.round(prediction)-a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "090c42ee-0c21-4a2a-8178-dac422a13003",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8bc71255a22e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "388b5bf9-f61b-4dd7-ac58-fc695b7435c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c3d8c93f720f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca22583e-76f3-47d2-a14d-b97b2b733595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a8bf2-f93b-470a-bcf5-5baa26737cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (System Environment)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
